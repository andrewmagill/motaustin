{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from configparser import ConfigParser\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from skimage.measure import compare_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "############\n",
    "\n",
    "CONFIG_PATH = './conf/config.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config\n",
    "#########\n",
    "\n",
    "config = ConfigParser()\n",
    "\n",
    "config.read(CONFIG_PATH)\n",
    "sequence_config = dict(config['Sequence'])\n",
    "tracking_config = dict(config['Tracking'])\n",
    "\n",
    "sequence_config['frame_rate'] = config.getint(\"Sequence\",\"frame_rate\")\n",
    "sequence_config['seq_length'] = config.getint(\"Sequence\",\"seq_length\")\n",
    "sequence_config['img_width'] = config.getint(\"Sequence\",\"img_width\")\n",
    "sequence_config['img_height'] = config.getint(\"Sequence\",\"img_height\")\n",
    "\n",
    "tracking_config['scaled_height'] = config.getint(\"Tracking\",\"scaled_height\")\n",
    "tracking_config['scaled_width'] = config.getint(\"Tracking\",\"scaled_width\")\n",
    "tracking_config['memory'] = config.getint(\"Tracking\",\"memory\")\n",
    "tracking_config['similarity_threshold'] = config.getfloat(\"Tracking\",\"similarity_threshold\")\n",
    "tracking_config['distance_threshold'] = config.getfloat(\"Tracking\",\"distance_threshold\")\n",
    "tracking_config['confidence_threshold'] = config.getfloat(\"Tracking\",\"confidence_threshold\")\n",
    "\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input\n",
    "########\n",
    "\n",
    "# pos       name                    description\n",
    "# 1      frame number           frame in which the object is present\n",
    "# 2      identity number        trajectory id (-1 default for no track)\n",
    "# 3      bounding box x         x value from top left of bounding box\n",
    "# 4      bounding box y         y value from top left of bounding box\n",
    "# 5      bounding box width     width of bounding box in pixels\n",
    "# 6      bounding box height    height of bounding box in pixels\n",
    "# 7      confidence score      class detection confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "#########\n",
    "\n",
    "# pos       name                    description\n",
    "# 1      frame number           frame in which the object is present\n",
    "# 2      identity number        trajectory id (-1 default for no track)\n",
    "# 3      bounding box x         x value from top left of bounding box\n",
    "# 4      bounding box y         y value from top left of bounding box\n",
    "# 5      bounding box width     width of bounding box in pixels\n",
    "# 6      bounding box height    height of bounding box in pixels\n",
    "# 7      confidence score*      class detection confidence (gt: 1 or 0)\n",
    "# 8      class*                 type of class (1 for pedestrian)\n",
    "# 9      visibility*            percent visible (percent occluded = 1-visibility)\n",
    "\n",
    "#        *no need to output these values, will be ignore by evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_norm(obj1, obj2):\n",
    "        p1, p2 = None, None\n",
    "        \n",
    "        if type(obj1) == Box:\n",
    "            p1 = obj1.centroid\n",
    "        elif type(obj1) == Point:\n",
    "            p1 = obj1\n",
    "        \n",
    "        if type(obj1) == Box:\n",
    "            p2 = obj2.centroid\n",
    "        elif type(obj1) == Point:\n",
    "            p2 = obj2\n",
    "        \n",
    "        return math.sqrt((p2.x-p1.x)**2 + (p2.y-p1.y)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(obj1, obj2):\n",
    "        \n",
    "        if type(obj1) == Box and type(obj2) == Box:\n",
    "            \n",
    "            # just to make this a little more intuitive\n",
    "            box1, box2 = obj1, obj2\n",
    "            \n",
    "            # to make this easier we'll create two arrays, [x1, y1, x2, y2], s.t.\n",
    "            # (x1, y1) is the top left point for a box and (x2, y2) is the bottom right\n",
    "            a = [box1.x, box1.y, box1.x + box1.w, box1.y + box1.h]\n",
    "            b = [box2.x, box2.y, box2.x + box2.w, box2.y + box2.h]\n",
    "            \n",
    "            # intersection\n",
    "            \n",
    "            # find the boundary of the intersection between the two boxes\n",
    "            x1 = max(a[0], b[0]) # rightmost x of the top left points\n",
    "            y1 = max(a[1], b[1]) # lowest y of the top left points\n",
    "            x2 = min(a[2], b[2]) # leftmost x of the bottom right points\n",
    "            y2 = min(a[3], b[3]) # highest y of the bottom right points\n",
    "            \n",
    "            # find the area of the intersection\n",
    "            width = (x2 - x1)\n",
    "            height = (y2 - y1)\n",
    "            \n",
    "            # if no overlap don't bother going further, return 0\n",
    "            if width <= 0 or height <= 0:\n",
    "                return 0\n",
    "            \n",
    "            area_of_intersection = width * height\n",
    "            \n",
    "            # area of union\n",
    "            \n",
    "            # this is easy, you don't need to know where the boxes are, since you've\n",
    "            # already calculated the intersection. if you just add the total area\n",
    "            # of box_a and the the area of box_b you've counted the intersection\n",
    "            # twice, so just subtract the intersection once and you have the answer\n",
    "            a_area = (a[2] - a[0]) * (a[3] - a[1])\n",
    "            b_area = (b[2] - b[0]) * (b[3] - b[1])\n",
    "            \n",
    "            area_of_union = a_area + b_area - area_of_intersection\n",
    "            \n",
    "            # protect again division by zero\n",
    "            epsilon = 1e-5\n",
    "            \n",
    "            iou = area_of_intersection / (area_of_union + epsilon)\n",
    "            return iou\n",
    "    \n",
    "        elif type(obj1) == numpy.ndarray and type(obj2) == numpy.ndarray:\n",
    "            descriptors1, descriptors2 = obj1, obj2\n",
    "            \n",
    "            # brute force feature matching using manhattan distance\n",
    "        \n",
    "            bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "            matches = bf.match(descriptors_1, descriptors_2)\n",
    "            matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "            # TODO do we want to cutoff at a threshold?\n",
    "            \n",
    "            intersection = len(matches)\n",
    "            union = len(descriptors1)+len(descriptors2)-intersection\n",
    "\n",
    "            epsilon = 1e-5\n",
    "            \n",
    "            return intersection / (union+epsilon)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sift(box1, box2):\n",
    "        \n",
    "        img_path = sequence_config['img_path']\n",
    "        \n",
    "        # get filepaths\n",
    "        \n",
    "        img1_path = os.path.join(img_path, \"%06d.jpg\" % box1.frame)\n",
    "        img2_path = os.path.join(img_path, \"%06d.jpg\" % box2.frame)\n",
    "        \n",
    "        # read image files as grayscale\n",
    "        \n",
    "        color_img1 = cv2.imread(img1_path)\n",
    "        color_img2 = cv2.imread(img2_path)\n",
    "        \n",
    "        gray_img1 = cv2.cvtColor(color_img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray_img2 = cv2.cvtColor(color_img2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # cut out the firt object from the first image\n",
    "        \n",
    "        p1, p2 = box1.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop1 = gray_img1[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # cut out the second object from the second image\n",
    "        \n",
    "        p1, p2 = box2.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop2 = gray_img2[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # TODO consider experimenting with sharpening, contrast\n",
    "        \n",
    "        # find keypoints and descriptors        \n",
    "        # https://docs.opencv.org/4.3.0/da/df5/tutorial_py_sift_intro.html\n",
    "        # https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/\n",
    "        \n",
    "        try:\n",
    "            keypoints_1, descriptors_1 = sift.detectAndCompute(crop1,None)        \n",
    "            keypoints_2, descriptors_2 = sift.detectAndCompute(crop2,None)\n",
    "        except Exception as ex:\n",
    "            print(\"\\n\\nBox1: %s, \\nBox 2: %s\\n%s\\n\" % (str(box1), str(box2), str(ex)))\n",
    "            return 0 \n",
    "\n",
    "        if len(keypoints_1) == 0 or len(keypoints_2) == 0:\n",
    "            # sift may not detect keypoints in an image with a dark \n",
    "            # foreground on a dark background, or a light foreground\n",
    "            # light background. in this case, equalize the histogram\n",
    "            # to enhance contrast, and try again.\n",
    "            \n",
    "            try:\n",
    "                equ1 = cv2.equalizeHist(crop1)\n",
    "                equ2 = cv2.equalizeHist(crop2)\n",
    "                keypoints_1, descriptors_1 = sift.detectAndCompute(equ1,None)        \n",
    "                keypoints_2, descriptors_2 = sift.detectAndCompute(equ2,None)\n",
    "                \n",
    "                print(len(keypoints_1), len(keypoints_2))\n",
    "            except Exception as ex:\n",
    "                # print(\"\\n\\nBox1: %s, \\nBox 2: %s\\n%s\\n\" % (str(box1), str(box2), str(ex)))                        \n",
    "                return 0 \n",
    "        \n",
    "        # FLANN feature matcher\n",
    "        \n",
    "        try:\n",
    "            matches = flann.knnMatch(descriptors_1,descriptors_2,k=2)\n",
    "        except Exception as ex:\n",
    "            # print(box1, keypoints_1, box2, keypoints_2)\n",
    "            return 0\n",
    "\n",
    "        good_matches = []\n",
    "\n",
    "        # ratio test as per Lowe's paper\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good_matches.append(m)\n",
    "        \n",
    "        epsilon = 1e-5\n",
    "        \n",
    "        try:\n",
    "            return len(good_matches) / (len(matches)+epsilon)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "    @staticmethod\n",
    "    def ssim(box1, box2):\n",
    "        \n",
    "        img_path = sequence_config['img_path']\n",
    "        \n",
    "        # get filepaths\n",
    "        \n",
    "        img1_path = os.path.join(img_path, \"%06d.jpg\" % box1.frame)\n",
    "        img2_path = os.path.join(img_path, \"%06d.jpg\" % box2.frame)\n",
    "        \n",
    "        # read image files as grayscale\n",
    "        \n",
    "        color_img1 = cv2.imread(img1_path)\n",
    "        color_img2 = cv2.imread(img2_path)\n",
    "        \n",
    "        gray_img1 = cv2.cvtColor(color_img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray_img2 = cv2.cvtColor(color_img2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # cut out the firt object from the first image\n",
    "        \n",
    "        p1, p2 = box1.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop1 = gray_img1[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # cut out the second object from the second image\n",
    "        \n",
    "        p1, p2 = box2.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop2 = gray_img2[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        try:\n",
    "            eq1 = crop1 # cv2.equalizeHist(crop1)\n",
    "            eq2 = crop2 # cv2.equalizeHist(crop2)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return 0\n",
    "\n",
    "        dim = (tracking_config[\"scaled_width\"], tracking_config[\"scaled_height\"])\n",
    "                \n",
    "        try:\n",
    "            scaled1 = cv2.resize(eq1, dim, interpolation=cv2.INTER_AREA)\n",
    "            scaled2 = cv2.resize(eq2, dim, interpolation=cv2.INTER_AREA)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            score, diff = compare_ssim(scaled1, scaled2, full=True)\n",
    "            # diff = (diff*255).astype(\"unit8\") # interesting, but I don't need this information\n",
    "            return score\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box(object):\n",
    "    def __init__(self, x, y, w, h, index=None, frame=None, track_id=None, conf=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.index = index\n",
    "        self.frame = frame\n",
    "        self.track_id = track_id\n",
    "        self.conf = conf\n",
    "    \n",
    "    @property\n",
    "    def coords(self):\n",
    "        return(Point(self.x, self.y), Point(self.x+self.w, self.y+self.h))\n",
    "    \n",
    "    @property\n",
    "    def centroid(self):\n",
    "        return Point(self.x+(self.w*0.5), self.y+(self.h*0.5))\n",
    "\n",
    "    def copy(self, offset_x=0, offset_y=0):\n",
    "        \n",
    "        # make sure you don't go outside the image bounds\n",
    "        # i'm sure there's a more elegant way of doing this\n",
    "        \n",
    "        x1, y1 = self.x+offset_x, self.y+offset_y\n",
    "        x2, y2 = x1+self.w, y1+self.h\n",
    "        \n",
    "        image_width = sequence_config['img_width']\n",
    "        image_height = sequence_config['img_height']\n",
    "        w, h = self.w, self.h\n",
    "        \n",
    "        if x1 < 0:\n",
    "            x1 = 0\n",
    "        if y1 < 0:\n",
    "            y1 = 0\n",
    "        if x2 > image_width:\n",
    "            w = image_width - self.x\n",
    "        if y2 > image_height:\n",
    "            h = image_height - self.y\n",
    "        \n",
    "        return Box(x1, y1, w, h, self.index, self.frame, self.track_id, self.conf)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Box(%s, %s, %s, %s, %s, %s, %s, %s)\" % (\n",
    "            self.x, self.y, self.w, self.h, self.index, self.frame, self.track_id, self.conf\n",
    "        )\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"index: %s, frame: %s, track: %s, x: %s, y: %s, w: %s, h: %s, conf: %s\" % (\n",
    "            self.index, self.frame, self.track_id, self.x, self.y, self.w, self.h, self.conf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track(object):\n",
    "    counter = 1\n",
    "    \n",
    "    def __init__(self, o):\n",
    "        self.id = Track.counter\n",
    "        Track.counter += 1\n",
    "        \n",
    "        if type(o) == list:\n",
    "            boxes = o\n",
    "            self.boxes = boxes\n",
    "        elif type(o) == Box:\n",
    "            box = o\n",
    "            self.boxes = [box]\n",
    "            \n",
    "        self.is_active = True\n",
    "    \n",
    "    def add(self, box):\n",
    "        box.track_id = self.id\n",
    "        self.boxes.append(box)\n",
    "    \n",
    "    @staticmethod\n",
    "    def angle(box1, box2):\n",
    "        \n",
    "        p1 = box1.centroid\n",
    "        p2 = box2.centroid\n",
    "\n",
    "        rads = math.atan2(p1.y-p2.y, p1.x-p2.x)\n",
    "        # deg = math.degrees(rads)\n",
    "        # return rads\n",
    "        return rads\n",
    "    \n",
    "    @staticmethod\n",
    "    def distance(box1, box2):\n",
    "            \n",
    "        p1 = box1.centroid\n",
    "        p2 = box2.centroid\n",
    "\n",
    "        distance = math.hypot((p2.x-p1.x),(p2.y-p1.y))\n",
    "        # distance = sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def predict(self, frame_id):\n",
    "        end_of_track = self.boxes[-1] # this is the last box added to the track\n",
    "        \n",
    "        frames_since_last_match = frame_id - end_of_track.frame\n",
    "        \n",
    "        if frames_since_last_match > tracking_config['memory']:\n",
    "            # forget this track if it's outside the memory window\n",
    "            return None\n",
    "        \n",
    "        if len(self.boxes)==1:\n",
    "            predicted_location = end_of_track.copy()\n",
    "        else:\n",
    "            second_to_end = self.boxes[-2]\n",
    "            \n",
    "            angle = Track.angle(second_to_end, end_of_track)\n",
    "            distance = Track.distance(second_to_end, end_of_track)\n",
    "            \n",
    "            # experiment, let's decrease the motion with each missing detection\n",
    "            \n",
    "            if frames_since_last_match < 1:\n",
    "                frames_since_last_match = 1\n",
    "                \n",
    "            if tracking_config['distance_measure'] == 'sift':\n",
    "                distance = 1\n",
    "            else:\n",
    "                distance = distance * 1/frames_since_last_match\n",
    "            \n",
    "            # end of experiment section\n",
    "            \n",
    "            offset_x = distance*math.sin(angle)\n",
    "            offset_y = distance*math.cos(angle)\n",
    "            \n",
    "            # predicted_location = current.copy() # match last location\n",
    "            predicted_location = end_of_track.copy(offset_x, offset_y) # estimate trajectory\n",
    "            \n",
    "        # predicted_location.frame += 1\n",
    "\n",
    "        return predicted_location\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Track([<__main__.Box>]) count: %s\" % len(self.boxes)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"TrackID: %s, Frames: %s-%s\" % (\n",
    "            self.id, self.boxes[0].frame, self.boxes[-1].frame\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame(object):\n",
    "    def __init__(self, number, boxes=None):\n",
    "        self.id = number\n",
    "        \n",
    "        if boxes is None:\n",
    "            boxes = []\n",
    "        \n",
    "        self._boxes = boxes\n",
    "    \n",
    "    def add(self, box):\n",
    "        if type(box) == Box:\n",
    "            self._boxes.append(box)\n",
    "        elif type(box) == list:\n",
    "            self._boxes.extend(boxes)\n",
    "    \n",
    "    @property\n",
    "    def objects(self):\n",
    "        return self._boxes\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Frame(%s, [<__main__.Box>]) count: %s\" % (self.id, len(self.boxes))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"FrameID: %s, Count: %s\" % (self.id, len(self.boxes))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detections(object):\n",
    "    def __init__(self, data_path=tracking_config['detections_path']):\n",
    "        self.data_path = data_path\n",
    "        self.mean_width = None\n",
    "        self.mean_height = None\n",
    "        self._df = None\n",
    "        self._frames = {}\n",
    "        self.start_index = None\n",
    "        self.end_index = None\n",
    "        self.count = sequence_config['seq_length']\n",
    "        self._pos = None\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self, conf_threshold=tracking_config['confidence_threshold']):\n",
    "        \n",
    "        header_list = ['frame','trajectory','x','y','w','h','confidence']\n",
    "        dtype = {'frame':int,'trajectory':int,'x':float,'y':float,'w':float,'h':float,'confidence':float}\n",
    "        \n",
    "        df = pd.read_csv(self.data_path, names=header_list, dtype=dtype)\n",
    "\n",
    "        # filter out low confidence detections\n",
    "        \n",
    "        self._df = df[df[\"confidence\"] >= conf_threshold]\n",
    "        \n",
    "        self.mean_width = self._df[\"w\"].mean()\n",
    "        self.mean_height = self._df[\"h\"].mean()\n",
    "\n",
    "        # get the indices for the first and last frames\n",
    "\n",
    "        self.start_index = self._df['frame'].min()\n",
    "        self.end_index = self._df['frame'].max() + 1\n",
    "\n",
    "        for i in range(self.start_index, self.end_index):\n",
    "            self._frames[i] = self._df.loc[df['frame']==i,:]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        while True:\n",
    "            if self._pos is None:\n",
    "                self._pos = self.start_index        \n",
    "            elif self._pos < self.end_index:\n",
    "                current, self._pos = self._pos, self._pos + 1\n",
    "                current_frame = self._frames[current]\n",
    "\n",
    "                detection_boxes = []\n",
    "\n",
    "                for i, detection in current_frame.iterrows():                \n",
    "                    frame_no, traj_no, x, y, w, h, conf = detection\n",
    "                    box = Box(x, y, w, h, int(i), int(frame_no), -1, conf)\n",
    "                    detection_boxes.append(box)\n",
    "\n",
    "                return Frame(current, detection_boxes)\n",
    "            else:\n",
    "                # self._pos = None\n",
    "                raise StopIteration()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Detections(%s, %s, %s, %s, %s, %s)\" % (\n",
    "            self.data_path, self.mean_width, self.mean_height, \n",
    "            self.start_index, self.end_index, self.count\n",
    "        )\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"path: %s, name: %s, mean_width: %s, mean_height: %s, start_index: %s, end_index: %s, count: %s)\" % (\n",
    "            self.data_path, sequence_config['name'], \n",
    "            self.mean_width, self.mean_height, \n",
    "            self.start_index, self.end_index, self.count\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO this is really the length of the frames, that's how\n",
    "        # I use it in trajectories... should rethink this\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectories(object):\n",
    "    def __init__(self):\n",
    "        # reset track counter\n",
    "        Track.counter = 1\n",
    "        self._tracks = {}\n",
    "        self.detections = Detections()\n",
    "        \n",
    "        self.dists = []\n",
    "        self.jaccards = []\n",
    "        self.sifts = []\n",
    "        self.ssims = []\n",
    "    \n",
    "    def add(self, o):\n",
    "        if type(o) == Box:\n",
    "            box = o\n",
    "            track = Track(box)\n",
    "            box.track_id = track.id\n",
    "            self._tracks[track.id] = track\n",
    "        if type(o) == Track:\n",
    "            track = o\n",
    "            self._tracks[track.id] = track\n",
    "    \n",
    "    def _ssid_matching(self, predictions, current_frame):\n",
    "            similarity_vector = []\n",
    "            computed = []\n",
    "\n",
    "            for box_a in predictions:\n",
    "                for box_b in current_frame.objects:\n",
    "\n",
    "                    if box_a.index is None or box_b.index is None:\n",
    "                        raise Exception(\"Boundary index may not be None.\")                    \n",
    "\n",
    "                    this_pair = set([box_a.index, box_b.index])\n",
    "\n",
    "                    if this_pair not in computed:\n",
    "\n",
    "                        distance_threshold = tracking_config['distance_threshold']\n",
    "                        similarity_threshold = tracking_config['similarity_threshold']\n",
    "\n",
    "                        distance = Distance.l2_norm(box_a, box_b)\n",
    "                        # distance_ratio = (distance_threshold - distance) / distance_threshold\n",
    "\n",
    "                        if distance <= distance_threshold:\n",
    "\n",
    "                            similarity1 = Distance.ssim(box_a, box_b)\n",
    "                            similarity2 = Distance.sift(box_a, box_b)                            \n",
    "                            # mean_sim = (similarity1 + similarity2)/2\n",
    "                            \n",
    "                            # jaccard = Distance.jaccard(box_a, box_b)\n",
    "                            # mean_dist = (distance_ratio+jaccard) / 2\n",
    "\n",
    "                            # self.dists.append(distance_ratio)\n",
    "                            # self.jaccards.append(jaccard)\n",
    "                            self.sifts.append(similarity2)\n",
    "                            self.ssims.append(similarity1)\n",
    "                            \n",
    "#                             if mean_sim >= similarity_threshold:\n",
    "\n",
    "#                                 similarity_vector.append((mean_sim, box_a, box_b))\n",
    "                            \n",
    "#                             elif mean_dist >= 0.6:\n",
    "                                \n",
    "#                                 similarity_vector.append((mean_sim/2.1, box_a, box_b))\n",
    "\n",
    "                            if similarity1 >= similarity_threshold:\n",
    "\n",
    "                                similarity_vector.append((similarity1, box_a, box_b))\n",
    "\n",
    "                        computed.append(this_pair)\n",
    "\n",
    "            similarity_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            matching_pairs = []\n",
    "            matched_objects = set([])\n",
    "\n",
    "            for distance, box_1, box_2 in similarity_vector:\n",
    "\n",
    "                # we want to ensure that we only return the top matches, so we keep track\n",
    "                # of the objects already matched and ignore less similar matches for these objects\n",
    "\n",
    "                if box_1.index not in matched_objects and box_2.index not in matched_objects:                \n",
    "\n",
    "                    match = (box_1, box_2)\n",
    "                    matching_pairs.append(match)\n",
    "\n",
    "                    matched_objects.add(box_1.index)\n",
    "                    matched_objects.add(box_2.index)\n",
    "\n",
    "            # unmatched detections in current frame need to become new tracks\n",
    "\n",
    "            if len(self.ssims)%100 == 0:                \n",
    "            \n",
    "                bins = np.linspace(0,1,30)\n",
    "                plt.hist([self.jaccards,self.dists], bins, label=['IOU','L2'])  # `density=False` would make counts\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.show()\n",
    "\n",
    "                plt.hist([self.sifts,self.ssims], bins, label=['SIFT','SSIM'])  # `density=False` would make counts\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.show()\n",
    "            \n",
    "            #####\n",
    "            \n",
    "            unmatched = set(\n",
    "                [box for box in current_frame.objects if box.index not in matched_objects]\n",
    "            )\n",
    "\n",
    "            for box in unmatched:\n",
    "               self.add(box)\n",
    "\n",
    "            print(\"%s:%s/%s\" % (current_frame.id, len(matching_pairs), len(self._tracks),), end =\" \")\n",
    "\n",
    "            return matching_pairs\n",
    "        \n",
    "    \n",
    "    def _sift_matching(self, predictions, current_frame):\n",
    "        measure = tracking_config['distance_measure']\n",
    "        \n",
    "        if measure != 'sift':\n",
    "            return self._measure(predictions, current_frame)\n",
    "                \n",
    "        similarity_vector = []\n",
    "        computed = []\n",
    "        \n",
    "        for box_a in predictions:\n",
    "            for box_b in current_frame.objects:\n",
    "                \n",
    "                if box_a.index is None or box_b.index is None:\n",
    "                    raise Exception(\"Boundary index may not be None.\")                    \n",
    "                    \n",
    "                this_pair = set([box_a.index, box_b.index])\n",
    "                \n",
    "                if this_pair not in computed:\n",
    "                    \n",
    "                    distance_threshold = tracking_config['distance_threshold']\n",
    "                    similarity_threshold = tracking_config['similarity_threshold']\n",
    "\n",
    "                    distance = Distance.l2_norm(box_a, box_b)\n",
    "                    distance_ratio = (distance_threshold - distance) / distance_threshold\n",
    "                                        \n",
    "                    if distance <= distance_threshold:\n",
    "                        \n",
    "                        similarity = Distance.sift(box_a, box_b)\n",
    "                        jaccard = Distance.jaccard(box_a, box_b)\n",
    "                        \n",
    "                        # if similarity > similarity_threshold:\n",
    "                            \n",
    "                        # little experiment to help match dark low contrast patches\n",
    "                        distance_weight = 1 # multiplier to increase or decrease importance of distance\n",
    "                        similarity_weight = 1 # multiplier to increase or decrease importance of image similarity\n",
    "                        jaccard_weight = 1\n",
    "\n",
    "                        mean_similarity = (\n",
    "                            (similarity_weight*similarity + distance_weight*distance_ratio + jaccard_weight*jaccard) / (similarity_weight+distance_weight+jaccard_weight)\n",
    "                        )                            \n",
    "\n",
    "                        similarity_vector.append((mean_similarity, box_a, box_b))\n",
    "                        \n",
    "                    computed.append(this_pair)\n",
    "\n",
    "        similarity_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        matching_pairs = []\n",
    "        matched_objects = set([])\n",
    "\n",
    "        for distance, box_1, box_2 in similarity_vector:\n",
    "            \n",
    "            # we want to ensure that we only return the top matches, so we keep track\n",
    "            # of the objects already matched and ignore less similar matches for these objects\n",
    "            \n",
    "            if box_1.index not in matched_objects and box_2.index not in matched_objects:                \n",
    "            \n",
    "                match = (box_1, box_2)\n",
    "                matching_pairs.append(match)\n",
    "                \n",
    "                matched_objects.add(box_1.index)\n",
    "                matched_objects.add(box_2.index)\n",
    "        \n",
    "        # unmatched detections in current frame need to become new tracks\n",
    "        \n",
    "        unmatched = set(\n",
    "            [box for box in current_frame.objects if box.index not in matched_objects]\n",
    "        )\n",
    "        \n",
    "        for box in unmatched:\n",
    "           self.add(box)\n",
    "        \n",
    "        print(\"%s:%s/%s\" % (current_frame.id, len(matching_pairs), len(self._tracks),), end =\" \")\n",
    "        \n",
    "        return matching_pairs\n",
    "    \n",
    "    def _matching(self, predictions, current_frame):\n",
    "        measure = tracking_config['distance_measure']\n",
    "        \n",
    "        if mesaure == 'sift':\n",
    "            distance_func = Distance.sift\n",
    "        elif measure == 'euclidean':\n",
    "            distance_func = Distance.l2_norm\n",
    "        elif measure == 'iou':\n",
    "            distance_func = Distance.jaccard\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "        sim_vector = []\n",
    "        computed = []\n",
    "        \n",
    "        for box_a in predictions:\n",
    "            for box_b in current_frame.objects:\n",
    "                \n",
    "                if box_a.index is None or box_b.index is None:\n",
    "                    raise Exception(\"Boundary index may not be None.\")                    \n",
    "                    \n",
    "                this_pair = set([box_a.index, box_b.index])\n",
    "                \n",
    "                if this_pair not in computed:\n",
    "                    \n",
    "                    similarity = distance_func(box_a, box_b)\n",
    "                    \n",
    "                    sim_threshold = tracking_config['similarity_threshold']\n",
    "                    \n",
    "                    if measure == 'euclidean':\n",
    "                        if similarity <= sim_threshold:\n",
    "                            \n",
    "                            ### little experiment, let's make it a ratio #############\n",
    "                            similarity = (sim_threshold - similarity) /  sim_threshold\n",
    "                            ##########################################################\n",
    "                            \n",
    "                            sim_vector.append((similarity, box_a, box_b))\n",
    "                    else:\n",
    "                        if similarity >= sim_threshold:\n",
    "                            sim_vector.append((similarity, box_a, box_b))\n",
    "                        \n",
    "                    computed.append(this_pair)\n",
    "\n",
    "        if measure == 'euclidean':            \n",
    "            # sim_vector.sort(key=lambda x: x[0])\n",
    "            sim_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "        else:\n",
    "            sim_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # print([x[0] for x in sim_vector[::3]], end=\", \")\n",
    "            \n",
    "        matching_pairs = []\n",
    "        matched_objects = set([])\n",
    "\n",
    "        for distance, box_1, box_2 in sim_vector:\n",
    "            \n",
    "            # we want to ensure that we only return the top matches, so we keep track\n",
    "            # of the objects already matched and ignore less similar matches for these objects\n",
    "            \n",
    "            if box_1.index not in matched_objects and box_2.index not in matched_objects:                \n",
    "            \n",
    "                match = (box_1, box_2)\n",
    "                matching_pairs.append(match)\n",
    "                \n",
    "                matched_objects.add(box_1.index)\n",
    "                matched_objects.add(box_2.index)\n",
    "        \n",
    "        # unmatched detections in current frame need to become new tracks\n",
    "        \n",
    "        unmatched = set(\n",
    "            [box for box in current_frame.objects if box.index not in matched_objects]\n",
    "        )\n",
    "        \n",
    "        for box in unmatched:\n",
    "            self.add(box)\n",
    "        \n",
    "        print(len(self._tracks), end =\" \")\n",
    "        \n",
    "        return matching_pairs\n",
    "    \n",
    "    def calculate(self, start=0, end=None):\n",
    "        if end is None:\n",
    "            end = len(self.detections)\n",
    "            \n",
    "        # for frame in self.detections[start:end]: # Detections not subscriptable\n",
    "        \n",
    "        for frame in self.detections:\n",
    "            # TODO fix this mess with dataframes\n",
    "            df = self.detections._frames[frame.id]\n",
    "            \n",
    "            if len(self._tracks)==0:\n",
    "                for box in frame.objects:\n",
    "                    self.add(box)\n",
    "                    \n",
    "                    # TODO fix this mess with dataframes\n",
    "                    df.loc[box.index, \"trajectory\"] = box.track_id\n",
    "                continue\n",
    "\n",
    "            predictions = []\n",
    "            for track_id, track in self._tracks.items():\n",
    "                prediction = track.predict(frame.id)\n",
    "                if prediction is not None:\n",
    "                    predictions.append(prediction)\n",
    "\n",
    "            #matching_pairs = self._matching(predictions, frame)\n",
    "            #matching_pairs = self._sift_matching(predictions, frame)\n",
    "            matching_pairs = self._ssid_matching(predictions, frame)\n",
    "            \n",
    "            for box_1, box_2 in matching_pairs:\n",
    "                track = self._tracks[box_1.track_id]\n",
    "                track.add(box_2)\n",
    "                \n",
    "                # TODO fix this mess with dataframes\n",
    "                df.loc[box_2.index, \"trajectory\"] = box_1.track_id\n",
    "\n",
    "    def output(self, path=None):\n",
    "        if path is None:            \n",
    "            for track_id, boxes in self._tracks.items():\n",
    "                print(track_id, boxes)\n",
    "        else:\n",
    "            # write\n",
    "            pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = Trajectories()\n",
    "trajectories.calculate()\n",
    "trajectories.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "colors = []\n",
    "offset = 50\n",
    "for i in range(100):\n",
    "    r = randint(0,255)\n",
    "    g = randint(0,255)\n",
    "    b = randint(0,255)\n",
    "    \n",
    "    if r < offset or g < offset or b < offset:\n",
    "        colors.append(((r,g,b),(r+offset, g+offset, b+offset)))\n",
    "    else:\n",
    "        colors.append(((r,g,b),(r-offset, g-offset, b-offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = []\n",
    "img_path = \"img1\"\n",
    "\n",
    "img_filenames = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "img_filenames.sort()\n",
    "\n",
    "for img_filename in img_filenames:\n",
    "    \n",
    "    if img_filename == '.empty':\n",
    "        continue\n",
    "    \n",
    "    frame_id = int(img_filename.split('.')[0])\n",
    "    frame = trajectories.detections._frames[frame_id]\n",
    "    img = cv2.imread(os.path.join(img_path, img_filename))\n",
    "    for index, (frame,track,x,y,w,h,conf) in frame.iterrows():\n",
    "        \n",
    "        track, x, y, w, h = int(track), int(x), int(y), int(w), int(h)\n",
    "        if track < 1:\n",
    "            continue\n",
    "        \n",
    "        color_index = track % 100\n",
    "        box_color, text_color = colors[color_index]\n",
    "        \n",
    "        top_left = (x, y)\n",
    "        bottom_right = (x+w, y+h)\n",
    "        thickness = 2\n",
    "        \n",
    "        cv2.rectangle(img, top_left, bottom_right, box_color, thickness)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        anchor_point = (x, y+20)\n",
    "        scale = 0.75\n",
    "        thickness = 2\n",
    "        line_type = cv2.LINE_AA\n",
    "        \n",
    "        cv2.putText(img, str(track), anchor_point, font, scale, text_color, thickness, line_type)\n",
    "        \n",
    "    video_frames.append(img)\n",
    "    \n",
    "output_path = 'output/%s.mp4' % tracking_config['out_seq_name']\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "fps = 30.0\n",
    "width = 1920\n",
    "height = 1080\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "for video_frame in video_frames:\n",
    "    video_writer.write(video_frame)\n",
    "\n",
    "cv2.imwrite('output/video_frame_1.jpeg', video_frames[1])\n",
    "    \n",
    "video_writer.release()\n",
    "video_frames.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/%s.txt' % tracking_config['out_seq_name']\n",
    "f=open(output_path,\"w\")\n",
    "for frame_id in range(1,sequence_config['seq_length']+1):\n",
    "    frame = trajectories.detections._frames[frame_id]\n",
    "    for index, (frame,track,x,y,w,h,conf) in frame.iterrows():\n",
    "        track, x, y, w, h = int(track), int(x), int(y), int(w), int(h)\n",
    "        if track < 1:\n",
    "            continue\n",
    "        f.write(\"%s,%s,%s,%s,%s,%s,%s,%s,%s\\n\"%(frame_id,track,x,y,w,h,1,1,conf))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance_measure': 'ssid',\n",
       " 'similarity_threshold': 0.2,\n",
       " 'distance_threshold': 120.0,\n",
       " 'confidence_threshold': 0.1,\n",
       " 'memory': 90,\n",
       " 'detections_path': 'det/det.txt',\n",
       " 'groudntruth_path': 'gt/gt.txt',\n",
       " 'scaled_width': 66,\n",
       " 'scaled_height': 180,\n",
       " 'out_seq_name': 'ssid_0.2_120_0.1_90_nomo_ssid_noeq'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracking_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0,1,30)\n",
    "plt.hist([trajectories.sifts,trajectories.ssims], bins, label=['SIFT','SSIM'])  # `density=False` would make counts\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.savefig('/Users/amagill/Downloads/sift_ssim_realworld.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
