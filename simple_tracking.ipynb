{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from configparser import ConfigParser\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "############\n",
    "\n",
    "CONFIG_PATH = './conf/config.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config\n",
    "#########\n",
    "\n",
    "config = ConfigParser()\n",
    "\n",
    "config.read(CONFIG_PATH)\n",
    "sequence_config = dict(config['Sequence'])\n",
    "tracking_config = dict(config['Tracking'])\n",
    "\n",
    "sequence_config['frame_rate'] = config.getint(\"Sequence\",\"frame_rate\")\n",
    "sequence_config['seq_length'] = config.getint(\"Sequence\",\"seq_length\")\n",
    "sequence_config['img_width'] = config.getint(\"Sequence\",\"img_width\")\n",
    "sequence_config['img_height'] = config.getint(\"Sequence\",\"img_height\")\n",
    "\n",
    "tracking_config['scaled_height'] = config.getint(\"Tracking\",\"scaled_height\")\n",
    "tracking_config['scaled_width'] = config.getint(\"Tracking\",\"scaled_width\")\n",
    "tracking_config['memory'] = config.getint(\"Tracking\",\"memory\")\n",
    "tracking_config['similarity_threshold'] = config.getfloat(\"Tracking\",\"similarity_threshold\")\n",
    "tracking_config['distance_threshold'] = config.getfloat(\"Tracking\",\"distance_threshold\")\n",
    "tracking_config['confidence_threshold'] = config.getfloat(\"Tracking\",\"confidence_threshold\")\n",
    "\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input\n",
    "########\n",
    "\n",
    "# pos       name                    description\n",
    "# 1      frame number           frame in which the object is present\n",
    "# 2      identity number        trajectory id (-1 default for no track)\n",
    "# 3      bounding box x         x value from top left of bounding box\n",
    "# 4      bounding box y         y value from top left of bounding box\n",
    "# 5      bounding box width     width of bounding box in pixels\n",
    "# 6      bounding box height    height of bounding box in pixels\n",
    "# 7      confidence score      class detection confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "#########\n",
    "\n",
    "# pos       name                    description\n",
    "# 1      frame number           frame in which the object is present\n",
    "# 2      identity number        trajectory id (-1 default for no track)\n",
    "# 3      bounding box x         x value from top left of bounding box\n",
    "# 4      bounding box y         y value from top left of bounding box\n",
    "# 5      bounding box width     width of bounding box in pixels\n",
    "# 6      bounding box height    height of bounding box in pixels\n",
    "# 7      confidence score*      class detection confidence (gt: 1 or 0)\n",
    "# 8      class*                 type of class (1 for pedestrian)\n",
    "# 9      visibility*            percent visible (percent occluded = 1-visibility)\n",
    "\n",
    "#        *no need to output these values, will be ignore by evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_norm(obj1, obj2):\n",
    "        p1, p2 = None, None\n",
    "        \n",
    "        if type(obj1) == Box:\n",
    "            p1 = obj1.centroid\n",
    "        elif type(obj1) == Point:\n",
    "            p1 = obj1\n",
    "        \n",
    "        if type(obj1) == Box:\n",
    "            p2 = obj2.centroid\n",
    "        elif type(obj1) == Point:\n",
    "            p2 = obj2\n",
    "        \n",
    "        return math.sqrt((p2.x-p1.x)**2 + (p2.y-p1.y)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(obj1, obj2):\n",
    "        \n",
    "        if type(obj1) == Box and type(obj2) == Box:\n",
    "            \n",
    "            # just to make this a little more intuitive\n",
    "            box1, box2 = obj1, obj2\n",
    "            \n",
    "            # to make this easier we'll create two arrays, [x1, y1, x2, y2], s.t.\n",
    "            # (x1, y1) is the top left point for a box and (x2, y2) is the bottom right\n",
    "            a = [box1.x, box1.y, box1.x + box1.w, box1.y + box1.h]\n",
    "            b = [box2.x, box2.y, box2.x + box2.w, box2.y + box2.h]\n",
    "            \n",
    "            # intersection\n",
    "            \n",
    "            # find the boundary of the intersection between the two boxes\n",
    "            x1 = max(a[0], b[0]) # rightmost x of the top left points\n",
    "            y1 = max(a[1], b[1]) # lowest y of the top left points\n",
    "            x2 = min(a[2], b[2]) # leftmost x of the bottom right points\n",
    "            y2 = min(a[3], b[3]) # highest y of the bottom right points\n",
    "            \n",
    "            # find the area of the intersection\n",
    "            width = (x2 - x1)\n",
    "            height = (y2 - y1)\n",
    "            \n",
    "            # if no overlap don't bother going further, return 0\n",
    "            if width <= 0 or height <= 0:\n",
    "                return 0\n",
    "            \n",
    "            area_of_intersection = width * height\n",
    "            \n",
    "            # area of union\n",
    "            \n",
    "            # this is easy, you don't need to know where the boxes are, since you've\n",
    "            # already calculated the intersection. if you just add the total area\n",
    "            # of box_a and the the area of box_b you've counted the intersection\n",
    "            # twice, so just subtract the intersection once and you have the answer\n",
    "            a_area = (a[2] - a[0]) * (a[3] - a[1])\n",
    "            b_area = (b[2] - b[0]) * (b[3] - b[1])\n",
    "            \n",
    "            area_of_union = a_area + b_area - area_of_intersection\n",
    "            \n",
    "            # protect again division by zero\n",
    "            epsilon = 1e-5\n",
    "            \n",
    "            iou = area_of_intersection / (area_of_union + epsilon)\n",
    "            return iou\n",
    "    \n",
    "        elif type(obj1) == numpy.ndarray and type(obj2) == numpy.ndarray:\n",
    "            descriptors1, descriptors2 = obj1, obj2\n",
    "            \n",
    "            # brute force feature matching using manhattan distance\n",
    "        \n",
    "            bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "            matches = bf.match(descriptors_1, descriptors_2)\n",
    "            matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "            # TODO do we want to cutoff at a threshold?\n",
    "            \n",
    "            intersection = len(matches)\n",
    "            union = len(descriptors1)+len(descriptors2)-intersection\n",
    "\n",
    "            epsilon = 1e-5\n",
    "            \n",
    "            return intersection / (union+epsilon)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sift(box1, box2):\n",
    "        \n",
    "        img_path = sequence_config['img_path']\n",
    "        \n",
    "        # get filepaths\n",
    "        \n",
    "        img1_path = os.path.join(img_path, \"%06d.jpg\" % box1.frame)\n",
    "        img2_path = os.path.join(img_path, \"%06d.jpg\" % box2.frame)\n",
    "        \n",
    "        # read image files as grayscale\n",
    "        \n",
    "        color_img1 = cv2.imread(img1_path)\n",
    "        color_img2 = cv2.imread(img2_path)\n",
    "        \n",
    "        gray_img1 = cv2.cvtColor(color_img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray_img2 = cv2.cvtColor(color_img2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # cut out the firt object from the first image\n",
    "        \n",
    "        p1, p2 = box1.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop1 = gray_img1[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # cut out the second object from the second image\n",
    "        \n",
    "        p1, p2 = box2.coords\n",
    "        start_x, start_y = int(round(p1.x)), int(round(p1.y))\n",
    "        end_x, end_y = int(round(p2.x)),int(round(p2.y))\n",
    "        \n",
    "        crop2 = gray_img2[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        # TODO consider experimenting with sharpening, contrast\n",
    "        \n",
    "        # find keypoints and descriptors        \n",
    "        # https://docs.opencv.org/4.3.0/da/df5/tutorial_py_sift_intro.html\n",
    "        # https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/\n",
    "\n",
    "        try:\n",
    "            keypoints_1, descriptors_1 = sift.detectAndCompute(crop1,None)        \n",
    "            keypoints_2, descriptors_2 = sift.detectAndCompute(crop2,None)\n",
    "        except Exception as ex:\n",
    "            # print(\"\\n\\nBox1: %s, \\nBox 2: %s\\n%s\\n\" % (str(box1), str(box2), str(ex)))                        \n",
    "            return 0 \n",
    "\n",
    "        # brute force feature matching using manhattan distance\n",
    "        # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n",
    "        \n",
    "#         # FLANN parameters\n",
    "#         FLANN_INDEX_KDTREE = 0\n",
    "#         index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "#         search_params = dict(checks=100)   # or pass empty dictionary\n",
    "\n",
    "#         flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "        try:\n",
    "            matches = flann.knnMatch(descriptors_1,descriptors_2,k=2)\n",
    "        except Exception as ex:\n",
    "            # print(box1, keypoints_1, box2, keypoints_2)\n",
    "            return 0\n",
    "\n",
    "        # Need to draw only good matches, so create a mask\n",
    "        good_matches = []\n",
    "\n",
    "        # ratio test as per Lowe's paper\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good_matches.append(m)\n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             matches = [m for m in bf.match(descriptors_1,descriptors_2) if m.distance < 300]\n",
    "#             # matches = sorted(matches, key = lambda x:x.distance)            \n",
    "#         except:\n",
    "#             print(\"\\n\\nerror matching descriptors\\n%s\\n%s\\n%s\\n%s\\n%s\\n%s\\n\\n\" % (str(box1),str(box2),str(keypoints_1), str(descriptors_1),str(keypoints_2), str(descriptors_2)))            \n",
    "#             return 0\n",
    "        \n",
    "#         from statistics import mean, median\n",
    "#         distances = [m.distance for m in matches]\n",
    "#         try:\n",
    "#             print(\"min: %s, max: %s, mean: %s, median: %s\" % (min(distances),max(distances),mean(distances),median(distances)))        \n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "        # find the jaccard coefficient for intersection over union (matches / descriptors)\n",
    "        \n",
    "        epsilon = 1e-5\n",
    "        \n",
    "        try:\n",
    "            return len(good_matches) / len(matches)+epsilon\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box(object):\n",
    "    def __init__(self, x, y, w, h, index=None, frame=None, track_id=None, conf=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.index = index\n",
    "        self.frame = frame\n",
    "        self.track_id = track_id\n",
    "        self.conf = conf\n",
    "    \n",
    "    @property\n",
    "    def coords(self):\n",
    "        return(Point(self.x, self.y), Point(self.x+self.w, self.y+self.h))\n",
    "    \n",
    "    @property\n",
    "    def centroid(self):\n",
    "        return Point(self.x+(self.w*0.5), self.y+(self.h*0.5))\n",
    "\n",
    "    def copy(self, offset_x=0, offset_y=0):\n",
    "        \n",
    "        # make sure you don't go outside the image bounds\n",
    "        # i'm sure there's a more elegant way of doing this\n",
    "        \n",
    "        x1, y1 = self.x+offset_x, self.y+offset_y\n",
    "        x2, y2 = x1+self.w, y1+self.h\n",
    "        \n",
    "        image_width = sequence_config['img_width']\n",
    "        image_height = sequence_config['img_height']\n",
    "        w, h = self.w, self.h\n",
    "        \n",
    "        if x1 < 0:\n",
    "            x1 = 0\n",
    "        if y1 < 0:\n",
    "            y1 = 0\n",
    "        if x2 > image_width:\n",
    "            w = image_width - self.x\n",
    "        if y2 > image_height:\n",
    "            h = image_height - self.y\n",
    "        \n",
    "        return Box(x1, y1, w, h, self.index, self.frame, self.track_id, self.conf)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Box(%s, %s, %s, %s, %s, %s, %s, %s)\" % (\n",
    "            self.x, self.y, self.w, self.h, self.index, self.frame, self.track_id, self.conf\n",
    "        )\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"index: %s, frame: %s, track: %s, x: %s, y: %s, w: %s, h: %s, conf: %s\" % (\n",
    "            self.index, self.frame, self.track_id, self.x, self.y, self.w, self.h, self.conf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track(object):\n",
    "    counter = 1\n",
    "    \n",
    "    def __init__(self, o):\n",
    "        self.id = Track.counter\n",
    "        Track.counter += 1\n",
    "        \n",
    "        if type(o) == list:\n",
    "            boxes = o\n",
    "            self.boxes = boxes\n",
    "        elif type(o) == Box:\n",
    "            box = o\n",
    "            self.boxes = [box]\n",
    "            \n",
    "        self.is_active = True\n",
    "    \n",
    "    def add(self, box):\n",
    "        box.track_id = self.id\n",
    "        self.boxes.append(box)\n",
    "    \n",
    "    @staticmethod\n",
    "    def angle(box1, box2):\n",
    "        \n",
    "        p1 = box1.centroid\n",
    "        p2 = box2.centroid\n",
    "\n",
    "        rads = math.atan2(p1.y-p2.y, p1.x-p2.x)\n",
    "        # deg = math.degrees(rads)\n",
    "        # return rads\n",
    "        return rads\n",
    "    \n",
    "    @staticmethod\n",
    "    def distance(box1, box2):\n",
    "            \n",
    "        p1 = box1.centroid\n",
    "        p2 = box2.centroid\n",
    "\n",
    "        distance = math.hypot((p2.x-p1.x),(p2.y-p1.y))\n",
    "        # distance = sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def predict(self, frame_id):\n",
    "        end_of_track = self.boxes[-1] # this is the last box added to the track\n",
    "        \n",
    "        frames_since_last_match = frame_id - end_of_track.frame\n",
    "        \n",
    "        if frames_since_last_match > tracking_config['memory']:\n",
    "            # forget this track if it's outside the memory window\n",
    "            return None\n",
    "        \n",
    "        if len(self.boxes)==1:\n",
    "            predicted_location = end_of_track.copy()\n",
    "        else:\n",
    "            second_to_end = self.boxes[-2]\n",
    "            \n",
    "            angle = Track.angle(second_to_end, end_of_track)\n",
    "            distance = Track.distance(second_to_end, end_of_track)\n",
    "            \n",
    "            # experiment, let's decrease the motion with each missing detection\n",
    "            \n",
    "            if frames_since_last_match < 1:\n",
    "                \n",
    "                frames_since_last_match = 1\n",
    "                print(\"Why frames_since_last_match < 1? From predict method in Track class.\")\n",
    "                \n",
    "            if tracking_config['distance_measure'] == 'sift':\n",
    "                distance = 1\n",
    "            else:\n",
    "                # distance = distance * 1/frames_since_last_match\n",
    "                distance = 1\n",
    "                \n",
    "            # end of experiment section\n",
    "            \n",
    "            offset_x = distance*math.sin(angle)\n",
    "            offset_y = distance*math.cos(angle)\n",
    "            \n",
    "            # predicted_location = current.copy() # match last location\n",
    "            predicted_location = end_of_track.copy(offset_x, offset_y) # estimate trajectory\n",
    "            \n",
    "        # predicted_location.frame += 1\n",
    "\n",
    "        return predicted_location\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Track([<__main__.Box>]) count: %s\" % len(self.boxes)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"TrackID: %s, Frames: %s-%s\" % (\n",
    "            self.id, self.boxes[0].frame, self.boxes[-1].frame\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame(object):\n",
    "    def __init__(self, number, boxes=None):\n",
    "        self.id = number\n",
    "        \n",
    "        if boxes is None:\n",
    "            boxes = []\n",
    "        \n",
    "        self._boxes = boxes\n",
    "    \n",
    "    def add(self, box):\n",
    "        if type(box) == Box:\n",
    "            self._boxes.append(box)\n",
    "        elif type(box) == list:\n",
    "            self._boxes.extend(boxes)\n",
    "    \n",
    "    @property\n",
    "    def objects(self):\n",
    "        return self._boxes\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Frame(%s, [<__main__.Box>]) count: %s\" % (self.id, len(self.boxes))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"FrameID: %s, Count: %s\" % (self.id, len(self.boxes))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detections(object):\n",
    "    def __init__(self, data_path=tracking_config['detections_path']):\n",
    "        self.data_path = data_path\n",
    "        self.mean_width = None\n",
    "        self.mean_height = None\n",
    "        self._df = None\n",
    "        self._frames = {}\n",
    "        self.start_index = None\n",
    "        self.end_index = None\n",
    "        self.count = sequence_config['seq_length']\n",
    "        self._pos = None\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self, conf_threshold=tracking_config['confidence_threshold']):\n",
    "        \n",
    "        header_list = ['frame','trajectory','x','y','w','h','confidence']\n",
    "        dtype = {'frame':int,'trajectory':int,'x':float,'y':float,'w':float,'h':float,'confidence':float}\n",
    "        \n",
    "        df = pd.read_csv(self.data_path, names=header_list, dtype=dtype)\n",
    "\n",
    "        # filter out low confidence detections\n",
    "        \n",
    "        self._df = df[df[\"confidence\"] >= conf_threshold]\n",
    "        \n",
    "        self.mean_width = self._df[\"w\"].mean()\n",
    "        self.mean_height = self._df[\"h\"].mean()\n",
    "\n",
    "        # get the indices for the first and last frames\n",
    "\n",
    "        self.start_index = self._df['frame'].min()\n",
    "        self.end_index = self._df['frame'].max() + 1\n",
    "\n",
    "        for i in range(self.start_index, self.end_index):\n",
    "            self._frames[i] = self._df.loc[df['frame']==i,:]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        while True:\n",
    "            if self._pos is None:\n",
    "                self._pos = self.start_index        \n",
    "            elif self._pos < self.end_index:\n",
    "                current, self._pos = self._pos, self._pos + 1\n",
    "                current_frame = self._frames[current]\n",
    "\n",
    "                detection_boxes = []\n",
    "\n",
    "                for i, detection in current_frame.iterrows():                \n",
    "                    frame_no, traj_no, x, y, w, h, conf = detection\n",
    "                    box = Box(x, y, w, h, int(i), int(frame_no), -1, conf)\n",
    "                    detection_boxes.append(box)\n",
    "\n",
    "                return Frame(current, detection_boxes)\n",
    "            else:\n",
    "                # self._pos = None\n",
    "                raise StopIteration()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Detections(%s, %s, %s, %s, %s, %s)\" % (\n",
    "            self.data_path, self.mean_width, self.mean_height, \n",
    "            self.start_index, self.end_index, self.count\n",
    "        )\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"path: %s, name: %s, mean_width: %s, mean_height: %s, start_index: %s, end_index: %s, count: %s)\" % (\n",
    "            self.data_path, sequence_config['name'], \n",
    "            self.mean_width, self.mean_height, \n",
    "            self.start_index, self.end_index, self.count\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO this is really the length of the frames, that's how\n",
    "        # I use it in trajectories... should rethink this\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectories(object):\n",
    "    def __init__(self):\n",
    "        # reset track counter\n",
    "        Track.counter = 1\n",
    "        self._tracks = {}\n",
    "        self.detections = Detections()\n",
    "    \n",
    "    def add(self, o):\n",
    "        if type(o) == Box:\n",
    "            box = o\n",
    "            track = Track(box)\n",
    "            box.track_id = track.id\n",
    "            self._tracks[track.id] = track\n",
    "        if type(o) == Track:\n",
    "            track = o\n",
    "            self._tracks[track.id] = track\n",
    "    \n",
    "    def _sift_matching(self, predictions, current_frame):\n",
    "        measure = tracking_config['distance_measure']\n",
    "        \n",
    "        if measure != 'sift':\n",
    "            return self._matching(predictions, current_frame)\n",
    "                \n",
    "        similarity_vector = []\n",
    "        computed = []\n",
    "                \n",
    "        for box_a in predictions:\n",
    "            for box_b in current_frame.objects:\n",
    "                \n",
    "                if box_a.index is None or box_b.index is None:\n",
    "                    raise Exception(\"Boundary index may not be None.\")                    \n",
    "                    \n",
    "                this_pair = set([box_a.index, box_b.index])\n",
    "                \n",
    "                if this_pair not in computed:\n",
    "                    \n",
    "                    distance = Distance.l2_norm(box_a, box_b)                    \n",
    "                    distance_threshold = tracking_config['distance_threshold']\n",
    "                    similarity_threshold = tracking_config['similarity_threshold']\n",
    "                    \n",
    "                    if distance <= distance_threshold:                        \n",
    "                        similarity = Distance.sift(box_a, box_b)\n",
    "                        if similarity > similarity_threshold:\n",
    "                            \n",
    "                            # little experiment to help match dark low contrast patches\n",
    "                            distance_weight = 1 # multiplier to increase or decrease importance of distance\n",
    "                            similarity_weight = 3 # multiplier to increase or decrease importance of image similarity\n",
    "                            \n",
    "                            # i'm giving more weight to similarity because i really only want proximity\n",
    "                            # to dominate when sift fails to detect keypoints\n",
    "                            \n",
    "                            distance_ratio = (distance_threshold - distance) / distance_threshold\n",
    "                            \n",
    "                            mean_similarity = (\n",
    "                                ((similarity_weight*similarity + distance_weight*distance_ratio)) / (similarity_weight+distance_weight)\n",
    "                            )                            \n",
    "                            \n",
    "                            # similarity_vector.append((mean_similarity, box_a, box_b))\n",
    "                            similarity_vector.append((similarity, box_a, box_b))\n",
    "                        \n",
    "                    computed.append(this_pair)\n",
    "\n",
    "        similarity_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        matching_pairs = []\n",
    "        matched_objects = set([])\n",
    "\n",
    "        for distance, box_1, box_2 in similarity_vector:\n",
    "            \n",
    "            # we want to ensure that we only return the top matches, so we keep track\n",
    "            # of the objects already matched and ignore less similar matches for these objects\n",
    "            \n",
    "            if box_1.index not in matched_objects and box_2.index not in matched_objects:                \n",
    "            \n",
    "                match = (box_1, box_2)\n",
    "                matching_pairs.append(match)\n",
    "                \n",
    "                matched_objects.add(box_1.index)\n",
    "                matched_objects.add(box_2.index)\n",
    "        \n",
    "        # unmatched detections in current frame need to become new tracks\n",
    "        \n",
    "        unmatched = set(\n",
    "            [box for box in current_frame.objects if box.index not in matched_objects]\n",
    "        )\n",
    "        \n",
    "        for box in unmatched:\n",
    "           self.add(box)\n",
    "        \n",
    "        print(\"%s/%s\" % (len(matching_pairs), len(self._tracks),), end =\" \")\n",
    "        \n",
    "        return matching_pairs\n",
    "    \n",
    "    def _matching(self, predictions, current_frame):\n",
    "        measure = tracking_config['distance_measure']\n",
    "        \n",
    "        if measure == 'sift':\n",
    "            distance_func = Distance.sift\n",
    "        elif measure == 'euclidean':\n",
    "            distance_func = Distance.l2_norm\n",
    "        elif measure == 'iou':\n",
    "            distance_func = Distance.jaccard\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "        sim_vector = []\n",
    "        computed = []\n",
    "        \n",
    "        for box_a in predictions:\n",
    "            for box_b in current_frame.objects:\n",
    "                \n",
    "                if box_a.index is None or box_b.index is None:\n",
    "                    raise Exception(\"Boundary index may not be None.\")                    \n",
    "                    \n",
    "                this_pair = set([box_a.index, box_b.index])\n",
    "                \n",
    "                if this_pair not in computed:\n",
    "                    \n",
    "                    similarity = distance_func(box_a, box_b)\n",
    "                    \n",
    "                    sim_threshold = tracking_config['similarity_threshold']\n",
    "                    \n",
    "                    if measure == 'euclidean':\n",
    "                        if similarity <= sim_threshold:\n",
    "                            \n",
    "                            # ## little experiment, let's make it a ratio #############\n",
    "                            # similarity = (sim_threshold - similarity) /  sim_threshold\n",
    "                            # ##########################################################\n",
    "                            \n",
    "                            sim_vector.append((similarity, box_a, box_b))\n",
    "                    else:\n",
    "                        if similarity >= sim_threshold:\n",
    "                            sim_vector.append((similarity, box_a, box_b))\n",
    "                        \n",
    "                    computed.append(this_pair)\n",
    "\n",
    "        if measure == 'euclidean':            \n",
    "            sim_vector.sort(key=lambda x: x[0])\n",
    "            # sim_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "        else:\n",
    "            sim_vector.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # print([x[0] for x in sim_vector[::3]], end=\", \")\n",
    "            \n",
    "        matching_pairs = []\n",
    "        matched_objects = set([])\n",
    "\n",
    "        for distance, box_1, box_2 in sim_vector:\n",
    "            \n",
    "            # we want to ensure that we only return the top matches, so we keep track\n",
    "            # of the objects already matched and ignore less similar matches for these objects\n",
    "            \n",
    "            if box_1.index not in matched_objects and box_2.index not in matched_objects:                \n",
    "            \n",
    "                match = (box_1, box_2)\n",
    "                matching_pairs.append(match)\n",
    "                \n",
    "                matched_objects.add(box_1.index)\n",
    "                matched_objects.add(box_2.index)\n",
    "        \n",
    "        # unmatched detections in current frame need to become new tracks\n",
    "        \n",
    "        unmatched = set(\n",
    "            [box for box in current_frame.objects if box.index not in matched_objects]\n",
    "        )\n",
    "        \n",
    "        for box in unmatched:\n",
    "            self.add(box)\n",
    "        \n",
    "        print(len(self._tracks), end =\" \")\n",
    "        \n",
    "        return matching_pairs\n",
    "    \n",
    "    def calculate(self, start=0, end=None):\n",
    "        if end is None:\n",
    "            end = len(self.detections)\n",
    "            \n",
    "        # for frame in self.detections[start:end]: # Detections not subscriptable\n",
    "        \n",
    "        for frame in self.detections:\n",
    "            # TODO fix this mess with dataframes\n",
    "            df = self.detections._frames[frame.id]\n",
    "            \n",
    "            if len(self._tracks)==0:\n",
    "                for box in frame.objects:\n",
    "                    self.add(box)\n",
    "                    \n",
    "                    # TODO fix this mess with dataframes\n",
    "                    df.loc[box.index, \"trajectory\"] = box.track_id\n",
    "                continue\n",
    "\n",
    "            predictions = []\n",
    "            for track_id, track in self._tracks.items():\n",
    "                prediction = track.predict(frame.id)\n",
    "                if prediction is not None:\n",
    "                    predictions.append(prediction)\n",
    "\n",
    "            #matching_pairs = self._matching(predictions, frame)\n",
    "            matching_pairs = self._sift_matching(predictions, frame)\n",
    "            \n",
    "            for box_1, box_2 in matching_pairs:\n",
    "                track = self._tracks[box_1.track_id]\n",
    "                track.add(box_2)\n",
    "                \n",
    "                # TODO fix this mess with dataframes\n",
    "                df.loc[box_2.index, \"trajectory\"] = box_1.track_id\n",
    "\n",
    "    def output(self, path=None):\n",
    "        if path is None:            \n",
    "            for track_id, boxes in self._tracks.items():\n",
    "                print(track_id, boxes)\n",
    "        else:\n",
    "            # write\n",
    "            pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosworth/.envs/three/motaustin/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 30 30 31 32 32 32 33 33 33 33 33 33 33 33 33 34 34 34 34 34 35 35 36 36 36 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 43 43 43 43 43 43 43 43 43 43 43 44 44 44 44 44 44 44 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 46 46 47 47 48 48 48 48 48 48 48 48 48 48 49 49 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 51 51 51 51 51 52 52 52 52 52 52 53 53 53 53 54 54 54 54 54 54 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 56 56 56 56 56 57 57 57 57 57 58 58 58 58 58 58 59 59 60 60 60 61 61 61 62 63 63 63 63 63 64 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 66 67 67 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 68 69 70 71 71 71 71 71 71 71 71 71 71 72 72 73 73 73 73 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 74 75 75 75 75 75 75 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 79 79 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 85 85 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 86 87 87 87 87 87 87 87 88 88 88 88 88 88 88 88 89 89 89 89 89 89 90 90 90 90 90 91 91 91 91 92 92 92 92 92 92 92 92 92 92 92 93 93 93 93 93 93 93 93 94 94 95 95 95 95 95 95 95 95 95 95 95 96 96 96 96 96 97 97 97 97 98 98 98 98 98 98 98 98 98 98 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 99 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 101 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 103 103 103 103 103 103 103 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 104 105 105 105 105 105 105 105 105 105 105 105 105 105 105 106 106 106 106 106 106 106 106 106 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 107 108 108 110 110 110 110 110 110 110 111 111 112 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 113 114 114 114 114 114 114 114 115 115 115 115 115 115 115 115 115 115 115 115 115 116 116 117 117 117 117 117 117 117 117 117 117 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 118 119 119 119 119 119 119 119 119 119 120 120 120 120 120 120 120 120 120 120 120 120 120 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 122 122 122 122 1 TrackID: 1, Frames: 1-89\n",
      "2 TrackID: 2, Frames: 1-325\n",
      "3 TrackID: 3, Frames: 1-1050\n",
      "4 TrackID: 4, Frames: 1-390\n",
      "5 TrackID: 5, Frames: 1-332\n",
      "6 TrackID: 6, Frames: 1-1050\n",
      "7 TrackID: 7, Frames: 1-8\n",
      "8 TrackID: 8, Frames: 1-66\n",
      "9 TrackID: 9, Frames: 1-38\n",
      "10 TrackID: 10, Frames: 1-1050\n",
      "11 TrackID: 11, Frames: 1-756\n",
      "12 TrackID: 12, Frames: 1-215\n",
      "13 TrackID: 13, Frames: 1-1050\n",
      "14 TrackID: 14, Frames: 1-1050\n",
      "15 TrackID: 15, Frames: 1-541\n",
      "16 TrackID: 16, Frames: 1-15\n",
      "17 TrackID: 17, Frames: 1-1050\n",
      "18 TrackID: 18, Frames: 1-781\n",
      "19 TrackID: 19, Frames: 1-174\n",
      "20 TrackID: 20, Frames: 1-635\n",
      "21 TrackID: 21, Frames: 1-136\n",
      "22 TrackID: 22, Frames: 1-895\n",
      "23 TrackID: 23, Frames: 1-21\n",
      "24 TrackID: 24, Frames: 1-17\n",
      "25 TrackID: 25, Frames: 1-37\n",
      "26 TrackID: 26, Frames: 1-3\n",
      "27 TrackID: 27, Frames: 4-187\n",
      "28 TrackID: 28, Frames: 14-527\n",
      "29 TrackID: 29, Frames: 28-420\n",
      "30 TrackID: 30, Frames: 36-560\n",
      "31 TrackID: 31, Frames: 38-229\n",
      "32 TrackID: 32, Frames: 39-213\n",
      "33 TrackID: 33, Frames: 42-177\n",
      "34 TrackID: 34, Frames: 51-167\n",
      "35 TrackID: 35, Frames: 56-569\n",
      "36 TrackID: 36, Frames: 58-61\n",
      "37 TrackID: 37, Frames: 61-109\n",
      "38 TrackID: 38, Frames: 66-315\n",
      "39 TrackID: 39, Frames: 76-109\n",
      "40 TrackID: 40, Frames: 105-582\n",
      "41 TrackID: 41, Frames: 129-166\n",
      "42 TrackID: 42, Frames: 158-571\n",
      "43 TrackID: 43, Frames: 161-390\n",
      "44 TrackID: 44, Frames: 172-334\n",
      "45 TrackID: 45, Frames: 187-220\n",
      "46 TrackID: 46, Frames: 203-244\n",
      "47 TrackID: 47, Frames: 205-221\n",
      "48 TrackID: 48, Frames: 207-217\n",
      "49 TrackID: 49, Frames: 217-345\n",
      "50 TrackID: 50, Frames: 219-602\n",
      "51 TrackID: 51, Frames: 238-385\n",
      "52 TrackID: 52, Frames: 243-1050\n",
      "53 TrackID: 53, Frames: 249-300\n",
      "54 TrackID: 54, Frames: 253-344\n",
      "55 TrackID: 55, Frames: 259-291\n",
      "56 TrackID: 56, Frames: 306-311\n",
      "57 TrackID: 57, Frames: 317-454\n",
      "58 TrackID: 58, Frames: 322-399\n",
      "59 TrackID: 59, Frames: 328-470\n",
      "60 TrackID: 60, Frames: 330-529\n",
      "61 TrackID: 61, Frames: 333-337\n",
      "62 TrackID: 62, Frames: 336-574\n",
      "63 TrackID: 63, Frames: 337-594\n",
      "64 TrackID: 64, Frames: 342-543\n",
      "65 TrackID: 65, Frames: 343-436\n",
      "66 TrackID: 66, Frames: 358-456\n",
      "67 TrackID: 67, Frames: 359-576\n",
      "68 TrackID: 68, Frames: 361-363\n",
      "69 TrackID: 69, Frames: 389-451\n",
      "70 TrackID: 70, Frames: 390-1027\n",
      "71 TrackID: 71, Frames: 391-1042\n",
      "72 TrackID: 72, Frames: 401-1050\n",
      "73 TrackID: 73, Frames: 403-584\n",
      "74 TrackID: 74, Frames: 407-428\n",
      "75 TrackID: 75, Frames: 423-785\n",
      "76 TrackID: 76, Frames: 437-535\n",
      "77 TrackID: 77, Frames: 446-1050\n",
      "78 TrackID: 78, Frames: 455-572\n",
      "79 TrackID: 79, Frames: 465-634\n",
      "80 TrackID: 80, Frames: 467-536\n",
      "81 TrackID: 81, Frames: 485-534\n",
      "82 TrackID: 82, Frames: 515-522\n",
      "83 TrackID: 83, Frames: 538-956\n",
      "84 TrackID: 84, Frames: 547-644\n",
      "85 TrackID: 85, Frames: 574-598\n",
      "86 TrackID: 86, Frames: 576-601\n",
      "87 TrackID: 87, Frames: 598-1050\n",
      "88 TrackID: 88, Frames: 605-803\n",
      "89 TrackID: 89, Frames: 613-679\n",
      "90 TrackID: 90, Frames: 619-628\n",
      "91 TrackID: 91, Frames: 624-1050\n",
      "92 TrackID: 92, Frames: 628-628\n",
      "93 TrackID: 93, Frames: 639-1050\n",
      "94 TrackID: 94, Frames: 647-720\n",
      "95 TrackID: 95, Frames: 649-1050\n",
      "96 TrackID: 96, Frames: 660-996\n",
      "97 TrackID: 97, Frames: 665-707\n",
      "98 TrackID: 98, Frames: 669-682\n",
      "99 TrackID: 99, Frames: 679-730\n",
      "100 TrackID: 100, Frames: 723-737\n",
      "101 TrackID: 101, Frames: 741-964\n",
      "102 TrackID: 102, Frames: 742-1050\n",
      "103 TrackID: 103, Frames: 786-817\n",
      "104 TrackID: 104, Frames: 793-1050\n",
      "105 TrackID: 105, Frames: 833-1050\n",
      "106 TrackID: 106, Frames: 847-1019\n",
      "107 TrackID: 107, Frames: 856-1050\n",
      "108 TrackID: 108, Frames: 875-1050\n",
      "109 TrackID: 109, Frames: 877-877\n",
      "110 TrackID: 110, Frames: 877-997\n",
      "111 TrackID: 111, Frames: 884-1050\n",
      "112 TrackID: 112, Frames: 886-1050\n",
      "113 TrackID: 113, Frames: 887-1050\n",
      "114 TrackID: 114, Frames: 941-1050\n",
      "115 TrackID: 115, Frames: 948-1050\n",
      "116 TrackID: 116, Frames: 961-967\n",
      "117 TrackID: 117, Frames: 963-1014\n",
      "118 TrackID: 118, Frames: 973-1050\n",
      "119 TrackID: 119, Frames: 995-1050\n",
      "120 TrackID: 120, Frames: 1004-1050\n",
      "121 TrackID: 121, Frames: 1017-1050\n",
      "122 TrackID: 122, Frames: 1047-1047\n"
     ]
    }
   ],
   "source": [
    "trajectories = Trajectories()\n",
    "trajectories.calculate()\n",
    "trajectories.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "colors = []\n",
    "offset = 50\n",
    "for i in range(100):\n",
    "    r = randint(0,255)\n",
    "    g = randint(0,255)\n",
    "    b = randint(0,255)\n",
    "    \n",
    "    if r < offset or g < offset or b < offset:\n",
    "        colors.append(((r,g,b),(r+offset, g+offset, b+offset)))\n",
    "    else:\n",
    "        colors.append(((r,g,b),(r-offset, g-offset, b-offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = []\n",
    "img_path = \"img1\"\n",
    "\n",
    "img_filenames = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "img_filenames.sort()\n",
    "\n",
    "for img_filename in img_filenames:\n",
    "    frame_id = int(img_filename.split('.')[0])\n",
    "    frame = trajectories.detections._frames[frame_id]\n",
    "    img = cv2.imread(os.path.join(img_path, img_filename))\n",
    "    for index, (frame,track,x,y,w,h,conf) in frame.iterrows():\n",
    "        \n",
    "        track, x, y, w, h = int(track), int(x), int(y), int(w), int(h)\n",
    "        if track < 1:\n",
    "            continue\n",
    "        \n",
    "        color_index = track % 100\n",
    "        box_color, text_color = colors[color_index]\n",
    "        \n",
    "        top_left = (x, y)\n",
    "        bottom_right = (x+w, y+h)\n",
    "        thickness = 2\n",
    "        \n",
    "        cv2.rectangle(img, top_left, bottom_right, box_color, thickness)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        anchor_point = (x, y+20)\n",
    "        scale = 0.75\n",
    "        thickness = 2\n",
    "        line_type = cv2.LINE_AA\n",
    "        \n",
    "        cv2.putText(img, str(track), anchor_point, font, scale, text_color, thickness, line_type)\n",
    "        \n",
    "    video_frames.append(img)\n",
    "\n",
    "output_path = 'output/%s.mp4' % tracking_config['out_seq_name']\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "fps = 30.0\n",
    "width = 1920\n",
    "height = 1080\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "for video_frame in video_frames:\n",
    "    video_writer.write(video_frame)\n",
    "\n",
    "video_writer.release()\n",
    "video_frames.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/%s.txt' % tracking_config['out_seq_name']\n",
    "f=open(output_path,\"w\")\n",
    "\n",
    "for frame_id in range(1,sequence_config['seq_length']+1):\n",
    "    frame = trajectories.detections._frames[frame_id]\n",
    "    for index, (frame,track,x,y,w,h,conf) in frame.iterrows():\n",
    "        \n",
    "        track, x, y, w, h = int(track), int(x), int(y), int(w), int(h)\n",
    "        if track < 1:\n",
    "            continue\n",
    "\n",
    "        f.write(\"%s,%s,%s,%s,%s,%s,%s,%s,%s\\n\"%(frame_id,track,x,y,w,h,1,1,conf))\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance_measure': 'euclidean',\n",
       " 'similarity_threshold': 20.0,\n",
       " 'distance_threshold': 20.0,\n",
       " 'confidence_threshold': 0.1,\n",
       " 'memory': 60,\n",
       " 'detections_path': 'det/det.txt',\n",
       " 'groudntruth_path': 'gt/gt.txt',\n",
       " 'scaled_width': 66,\n",
       " 'scaled_height': 180,\n",
       " 'out_seq_name': 'l2_20_0.1_60_nomo'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracking_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
